{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from matplotlib import dates\n",
    "import pingouin as pg\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set_theme(font_scale=0.7)\n",
    "sns.set_theme(rc={'savefig.dpi':300})\n",
    "# sns.set_theme(rc={\"figure.dpi\":300, 'savefig.dpi':300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of data to local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *This part is for the data extraction part.* <br>\n",
    "* *To open the link I used the urlopen with decode('ISO-8859-1') bcoz that's what let me download the data* <br>\n",
    "* *For some links, there is fixed columns 56, but for some there is quite more than that. So the try except block handles that* <br>\n",
    "* *For datasets that have columns greater than 56, I poped the last column and extracted the rest into a column and then build the dataframe from it.*\n",
    "* *I have also created the json file for each of the metedata*\n",
    "\n",
    "\n",
    "For Goddard\n",
    "\n",
    "I see two folders under the Goddard Space Flight Center location. They are Pandora2s1 & Pandora32s1. I suppose they are readings taken from different Pandora instruments numbered 2 & 32 respectively. Pandora2s1 holds the bigger dataset [by size] & also extends back to longer dates [from 2021]. <br>\n",
    "\n",
    "- For the NO2, So2, O3, Water Vapor I found the official data files. But for formaldehyde [HCHO] there is two files, but both of them are unvalidated: <br>\n",
    "https://data.pandonia-global-network.org/GreenbeltMD/Pandora2s1/L2/Pandora2s1_GreenbeltMD_L2_rfus5p1-8.txt -> Formaldehyde data are unvalidated [start date 2021/08/24] <br>\n",
    "https://data.pandonia-global-network.org/GreenbeltMD/Pandora2s1/L2/Pandora2s1_GreenbeltMD_L2_rfuh5p1-8.txt -> Formaldehyde data are unvalidated [start date 2021/09/15]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def getRawData(place, url_link_dict):\n",
    "    for chemical, link in url_link_dict.items():\n",
    "        with urllib.request.urlopen(link) as f:\n",
    "            html = f.read().decode('ISO-8859-1')\n",
    "\n",
    "        file_path_local = 'del.txt'\n",
    "        with open(file_path_local, 'w') as f:\n",
    "            f.write(html)\n",
    "\n",
    "        with open(file_path_local, 'r', encoding='ISO-8859-1') as f:\n",
    "            lines = f.readlines()    \n",
    "\n",
    "        line_index_to_find = '---------------------------------------------------------------------------------------\\n'\n",
    "        line_index = [ind for ind, val in enumerate(lines) if val == line_index_to_find]\n",
    "\n",
    "        metadata = dict()\n",
    "        metadata_column_slices = lines[:line_index[0]]\n",
    "        for column in metadata_column_slices:\n",
    "            key, value = column.split(':')[0].strip(), column.split(':')[1].strip()\n",
    "            metadata[key] = value\n",
    "\n",
    "        chemical_metadata_path = f'{metadata_path}/{chemical}_{place}.json'\n",
    "        with open(chemical_metadata_path, \"w\") as outfile: \n",
    "            json.dump(metadata, outfile)\n",
    "\n",
    "        try: \n",
    "            colums = []\n",
    "            columns_slices = lines[line_index[0]+1: line_index[1]]\n",
    "            for column in columns_slices:\n",
    "                col_name = column.split(':')[-1].strip()\n",
    "                colums.append(col_name)\n",
    "\n",
    "            final_values_list = []\n",
    "            data_slices = lines[line_index[1]+1:]\n",
    "            for data_row in data_slices:\n",
    "                data_row = data_row.replace('\\n', '')\n",
    "                values = data_row.split()\n",
    "                for i in range(1, len(values)):\n",
    "                    values[i] = float(values[i])\n",
    "                final_values_list.append(values)\n",
    "\n",
    "\n",
    "            df = pd.DataFrame(final_values_list, \n",
    "                            columns = colums) \n",
    "\n",
    "            df.reset_index(inplace=True)\n",
    "            csv_filepath = f'{data_folder_path}/{chemical}_{place}.csv'\n",
    "            df.to_csv(csv_filepath, index=False)  \n",
    "        except:\n",
    "            colums = []\n",
    "            columns_slices = lines[line_index[0]+1: line_index[1]]\n",
    "            for column in columns_slices:\n",
    "                col_name = column.split(':')[-1].strip()\n",
    "                colums.append(col_name)\n",
    "            colums.pop()\n",
    "\n",
    "            final_values_list = []\n",
    "            data_slices = lines[line_index[1]+1:]\n",
    "            for data_row in data_slices:\n",
    "                data_row = data_row.replace('\\n', '')\n",
    "                values = data_row.split()\n",
    "                print(len(values))\n",
    "                for i in range(1, len(values)):\n",
    "                    values[i] = float(values[i])\n",
    "                final_values_list.append(values[:len(colums)])\n",
    "\n",
    "            df = pd.DataFrame(final_values_list, \n",
    "                            columns = colums) \n",
    "\n",
    "            df.reset_index(inplace=True)\n",
    "            csv_filepath = f'{data_folder_path}/{chemical}_{place}.csv'\n",
    "            df.to_csv(csv_filepath, index=False)           \n",
    "\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data/Raw\"\n",
    "metadata_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data/Metadata\"\n",
    "\n",
    "# place = \"BeltsVille\"\n",
    "# BeltsVille_link_dict = {\n",
    "#     'NO2': 'https://data.pandonia-global-network.org/BeltsvilleMD/Pandora80s1/L2/Pandora80s1_BeltsvilleMD_L2_rnvs3p1-8.txt',\n",
    "#     'HCHO': 'https://data.pandonia-global-network.org/BeltsvilleMD/Pandora80s1/L2/Pandora80s1_BeltsvilleMD_L2_rfus5p1-8.txt',\n",
    "#     'SO2': 'https://data.pandonia-global-network.org/BeltsvilleMD/Pandora80s1/L2/Pandora80s1_BeltsvilleMD_L2_rsus1p1-8.txt',\n",
    "#     'H2O': 'https://data.pandonia-global-network.org/BeltsvilleMD/Pandora80s1/L2/Pandora80s1_BeltsvilleMD_L2_rwvt1p1-8.txt',\n",
    "#     'O3': 'https://data.pandonia-global-network.org/BeltsvilleMD/Pandora80s1/L2/Pandora80s1_BeltsvilleMD_L2_rout2p1-8.txt'\n",
    "# }\n",
    "# print(getRawData(place, BeltsVille_link_dict))\n",
    "\n",
    "\n",
    "# place = \"Goddard\"\n",
    "# Goddard_link_dict = {\n",
    "#     'NO2': 'https://data.pandonia-global-network.org/GreenbeltMD/Pandora2s1/L2/Pandora2s1_GreenbeltMD_L2_rnvs3p1-8.txt',\n",
    "#     'HCHO': 'https://data.pandonia-global-network.org/GreenbeltMD/Pandora2s1/L2/Pandora2s1_GreenbeltMD_L2_rfus5p1-8.txt',\n",
    "#     'SO2': 'https://data.pandonia-global-network.org/GreenbeltMD/Pandora2s1/L2/Pandora2s1_GreenbeltMD_L2_rsus1p1-8.txt',\n",
    "#     'H2O': 'https://data.pandonia-global-network.org/GreenbeltMD/Pandora2s1/L2/Pandora2s1_GreenbeltMD_L2_rwvt1p1-8.txt',\n",
    "#     'O3': 'https://data.pandonia-global-network.org/GreenbeltMD/Pandora2s1/L2/Pandora2s1_GreenbeltMD_L2_rout2p1-8.txt'\n",
    "# }\n",
    "# print(getRawData(place, Goddard_link_dict))\n",
    "\n",
    "\n",
    "# place = \"Mcmillan\"\n",
    "# Mcmillan_link_dict = {\n",
    "#     'NO2': 'https://data.pandonia-global-network.org/WashingtonDC/Pandora140s1/L2/Pandora140s1_WashingtonDC_L2_rnvs3p1-8.txt',\n",
    "#     'HCHO': 'https://data.pandonia-global-network.org/WashingtonDC/Pandora140s1/L2/Pandora140s1_WashingtonDC_L2_rfus5p1-8.txt',\n",
    "#     'SO2': 'https://data.pandonia-global-network.org/WashingtonDC/Pandora140s1/L2/Pandora140s1_WashingtonDC_L2_rsus1p1-8.txt',\n",
    "#     'H2O': 'https://data.pandonia-global-network.org/WashingtonDC/Pandora140s1/L2/Pandora140s1_WashingtonDC_L2_rwvt1p1-8.txt',\n",
    "#     'O3': 'https://data.pandonia-global-network.org/WashingtonDC/Pandora140s1/L2/Pandora140s1_WashingtonDC_L2_rout2p1-8.txt'\n",
    "# }\n",
    "# print(getRawData(place, Mcmillan_link_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of raw data into processed Data\n",
    "\n",
    "* renaming long column names into short ones. [quality column, vertical amount column] --> name changed\n",
    "* quality column codes changed into understandable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingDataset(place):\n",
    "    raw_csv_path = {}\n",
    "    for chemical in [\"H2O\", \"HCHO\", \"NO2\", \"O3\", \"SO2\"]:\n",
    "        raw_csv_path[chemical] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "\n",
    "    for chemical, file in raw_csv_path.items():\n",
    "        df = pd.read_csv(file)\n",
    "        df.drop('index', axis=1, inplace=True)\n",
    "\n",
    "        # rename of column names\n",
    "        df.rename(columns={quality_column_dict[chemical]: 'quality', vertical_amount_column_dict[chemical]: 'vertical_amount'}, inplace=True) \n",
    "        \n",
    "        ### renaming quality codes into understandable language. 10 -> high quality na, 11: 'medium quality na', 12: 'low quality na'\n",
    "        quality_dict = {10: 'high quality na', 11: 'medium quality na', 12: 'low quality na'} \n",
    "        df['quality'] = df['quality'].map(quality_dict)\n",
    "\n",
    "        ### Dealing with just 3 columms in here [datetime_UTC, quality, vertical_amount]\n",
    "        #### Change to pandas dateTime object\n",
    "        def change_iso8601_to_datetime_object(in_str):\n",
    "            return datetime.datetime.strptime(in_str, '%Y%m%dT%H%M%S.%fZ')\n",
    "\n",
    "        df['datetime_UTC'] = df['UT date and time for measurement center, yyyymmddThhmmssZ (ISO 8601)'].apply(change_iso8601_to_datetime_object) \n",
    "        ################################################################################\n",
    "        csv_filepath = f'/Users/ujjawal/Documents/Work/PandoraWork_2025/Data/{chemical}_{place}.csv'\n",
    "        df.to_csv(csv_filepath, index=False, date_format='%Y-%m-%d %H:%M:%S%z')        \n",
    "    \n",
    "\n",
    "quality_column_dict = {\n",
    "    'NO2': 'L2 data quality flag for nitrogen dioxide, 0=assured high quality, 1=assured medium quality, 2=assured low quality, 10=not-assured high quality, 11=not-assured medium quality, 12=not-assured low quality, 20=unusable high quality, 21=unusable medium quality, 22=unusable low quality',\n",
    "    'HCHO': 'L2 data quality flag for formaldehyde, 0=assured high quality, 1=assured medium quality, 2=assured low quality, 10=not-assured high quality, 11=not-assured medium quality, 12=not-assured low quality, 20=unusable high quality, 21=unusable medium quality, 22=unusable low quality',\n",
    "    'SO2':  'L2 data quality flag for sulfur dioxide, 0=assured high quality, 1=assured medium quality, 2=assured low quality, 10=not-assured high quality, 11=not-assured medium quality, 12=not-assured low quality, 20=unusable high quality, 21=unusable medium quality, 22=unusable low quality', \n",
    "    'H2O': 'L2 data quality flag for water vapor, 0=assured high quality, 1=assured medium quality, 2=assured low quality, 10=not-assured high quality, 11=not-assured medium quality, 12=not-assured low quality, 20=unusable high quality, 21=unusable medium quality, 22=unusable low quality',\n",
    "    'O3': 'L2 data quality flag for ozone, 0=assured high quality, 1=assured medium quality, 2=assured low quality, 10=not-assured high quality, 11=not-assured medium quality, 12=not-assured low quality, 20=unusable high quality, 21=unusable medium quality, 22=unusable low quality'\n",
    "}\n",
    "\n",
    "vertical_amount_column_dict = {\n",
    "    'NO2': 'Nitrogen dioxide total vertical column amount [moles per square meter], -9e99=retrieval not successful',\n",
    "    'HCHO': 'Formaldehyde total vertical column amount [moles per square meter], -9e99=retrieval not successful',\n",
    "    'SO2': 'Sulfur dioxide total vertical column amount [moles per square meter], -9e99=retrieval not successful',\n",
    "    'H2O': 'Water vapor total vertical column amount [moles per square meter], -9e99=retrieval not successful',\n",
    "    'O3': 'Ozone total vertical column amount [moles per square meter], -9e99=retrieval not successful'\n",
    "}\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data/Raw\"\n",
    "for place in ['BeltsVille', 'Goddard', 'Mcmillan']:\n",
    "    preprocessingDataset(place)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot and lineplot combined -> all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(csvPath): # returns a df object\n",
    "    '''\n",
    "    Filtering: Three columns, High Quality Data\n",
    "            @ get three column datatime_UTC, quality, vertical_amount\n",
    "            @ filter data with just the high quality data\n",
    "\n",
    "    - change datetime_UTC to EST\n",
    "    - date, year, month, hour, time, rounded_hours\n",
    "    - get the date extracted\n",
    "    - get the year extracted\n",
    "    - get the month extracted\n",
    "    - get the hour extracted\n",
    "    - get the time extracted \n",
    "    - get the rounded hours extracted\n",
    "    ------> return the df with these things and do the analysis\n",
    "    '''\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    #############################\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time\n",
    "\n",
    "    # def round_to_nearest_hour(timestamp):\n",
    "\n",
    "    #     if timestamp.minute >= 30 or (timestamp.minute == 29 and timestamp.second >= 30):\n",
    "    #         # Round up\n",
    "    #         rounded_timestamp = timestamp + timedelta(hours=1)\n",
    "    #         rounded_timestamp = rounded_timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "    #     else:\n",
    "    #         # Round down\n",
    "    #         rounded_timestamp = timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "    #     return rounded_timestamp\n",
    "    # df.loc[:, \"roundedHour\"] = df['datetime_EST'].apply(round_to_nearest_hour).dt.hour   \n",
    "    return df\n",
    "\n",
    "def scatterPlot_lineplot_combined(chemical, places_list):\n",
    "    preprocessed_csv_path = {}\n",
    "    for place in places_list:\n",
    "        preprocessed_csv_path[place] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "\n",
    "    for place, path in preprocessed_csv_path.items():\n",
    "        df = processDataset(path)\n",
    "        df['MovingAvg'] = df['vertical_amount'].rolling(window=1000).mean()\n",
    "        fig, axes = plt.subplots(2, 1, sharex=True, figsize=(10, 8))\n",
    "        \n",
    "        g = sns.lineplot(ax=axes[1], x=\"datetime_EST\", y=\"vertical_amount\", data=df, alpha=1, hue='Year', palette={2020:'darkgreen', 2021: 'pink', 2022: 'blue', 2023: 'red', 2024:'green', 2025:\"lightyellow\"})\n",
    "        # g = sns.lineplot(ax=axes[1], x='datetime_EST', y='MovingAvg', data=df, color=\"black\", label=\"Moving average of 1000 points\")  # uncomment when the black line is not needed. \n",
    "        g = sns.scatterplot(ax=axes[0],x=\"datetime_EST\", y=\"vertical_amount\", data=df, alpha=0.6, s=20, color='black')\n",
    "\n",
    "        # axes[1].xaxis.set_major_locator(dates.MonthLocator())\n",
    "        axes[1].xaxis.set_major_formatter(dates.DateFormatter('%b'))\n",
    "        axes[1].tick_params(axis='x', rotation=30)\n",
    "\n",
    "        sec_xaxis = axes[1].secondary_xaxis(-0.09)\n",
    "        sec_xaxis.xaxis.set_major_locator(dates.YearLocator())\n",
    "        sec_xaxis.xaxis.set_major_formatter(dates.DateFormatter('%Y'))\n",
    "        sec_xaxis.spines['bottom'].set_visible(False)\n",
    "\n",
    "        g.set(xlabel='', \n",
    "        ylabel='Vertical Column amount [moles per square meter',\n",
    "        title= f\"{place} {chemical} Plot\")\n",
    "\n",
    "        axes[1].set(xlabel='')\n",
    "        axes[1].set(ylabel='')\n",
    "\n",
    "        axes[0].set(ylabel='')\n",
    "        fig.supylabel(r\"Vertical Column Amount [moles/m$\\mathregular{^2}$]\", fontsize=12)\n",
    "        # plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/{chemical}/scatterPlotLinePlotCombined_{place}.png\"\n",
    "        # plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "        plt.show() \n",
    "        break\n",
    "\n",
    "\n",
    "# data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# chemical = \"H2O\"\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# scatterPlot_lineplot_combined(chemical, places_list)\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "for chemical in [\"H2O\", \"HCHO\", \"NO2\", \"O3\", \"SO2\"]:\n",
    "    places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "    scatterPlot_lineplot_combined(chemical, places_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly plot: Year & Places as a variable || with SE || Combined for all the places\n",
    "\n",
    "* *For each month, it gives in a mean value of the vertical amount* <br>\n",
    "* *For median, we can do this: estimator=np.median in `g = sns.lineplot()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(csvPath): # returns a df object\n",
    "    '''\n",
    "    Filtering: Three columns, High Quality Data\n",
    "            @ get three column datatime_UTC, quality, vertical_amount\n",
    "            @ filter data with just the high quality data\n",
    "\n",
    "    - change datetime_UTC to EST\n",
    "    - date, year, month, hour, time, rounded_hours\n",
    "    - get the date extracted\n",
    "    - get the year extracted\n",
    "    - get the month extracted\n",
    "    - get the hour extracted\n",
    "    - get the time extracted \n",
    "    - get the rounded hours extracted\n",
    "    ------> return the df with these things and do the analysis\n",
    "    '''\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    #############################\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time\n",
    "\n",
    "    # def round_to_nearest_hour(timestamp):\n",
    "\n",
    "    #     if timestamp.minute >= 30 or (timestamp.minute == 29 and timestamp.second >= 30):\n",
    "    #         # Round up\n",
    "    #         rounded_timestamp = timestamp + timedelta(hours=1)\n",
    "    #         rounded_timestamp = rounded_timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "    #     else:\n",
    "    #         # Round down\n",
    "    #         rounded_timestamp = timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "    #     return rounded_timestamp\n",
    "    # df.loc[:, \"roundedHour\"] = df['datetime_EST'].apply(round_to_nearest_hour).dt.hour   \n",
    "    return df\n",
    "\n",
    "\n",
    "def comparisonYearlyPlotWithSE(chemical, year, places_list):\n",
    "    preprocessed_csv_path = {}\n",
    "    for place in places_list:\n",
    "        preprocessed_csv_path[place] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "    \n",
    "    df_list = {}\n",
    "    for place, path in preprocessed_csv_path.items():\n",
    "        df = processDataset(path)\n",
    "        df = df[df.Year == year]\n",
    "        df_list[place] = df\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    for place, df in df_list.items():\n",
    "        g = sns.lineplot(x=df['Month'], y=df['vertical_amount'], ax=ax, err_style=\"bars\", errorbar=(\"se\"), c=color_dict[place], label=place)\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "    g.set(xlabel='', \n",
    "        ylabel=r\"Vertical Column Amount [moles/m$\\mathregular{^2}$]\",\n",
    "        title= f'{chemical} column amount comparison for year {year}')\n",
    "    if chemical == \"SO2\":\n",
    "        plt.ticklabel_format(style='plain', axis='y',useOffset=False)\n",
    "    plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/{chemical}/Comparision of {year}.png\"\n",
    "    plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "    plt.show();\n",
    "\n",
    "color_dict = {'BeltsVille': 'green',\n",
    "              'Goddard': 'blue',\n",
    "              'Mcmillan':'red'}\n",
    "\n",
    "\n",
    "# data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# for chemical in [\"H2O\",\"HCHO\", \"NO2\", \"O3\", \"SO2\"]:\n",
    "#     for year in [2023, 2024]:\n",
    "#         comparisonYearlyPlotWithSE(chemical, year, places_list)\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "for chemical in [\"SO2\"]:\n",
    "    for year in [2023, 2024]:\n",
    "        comparisonYearlyPlotWithSE(chemical, year, places_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual Trend -> Yearly plots on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(csvPath): # returns a df object\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time \n",
    "    return df\n",
    "\n",
    "\n",
    "def annualTrend(chemical, places_list, year_list):\n",
    "    preprocessed_csv_path = {}\n",
    "    for place in places_list:\n",
    "        preprocessed_csv_path[place] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "    \n",
    "    df_list_year = {} # year: df_year\n",
    "    for place, path in preprocessed_csv_path.items():\n",
    "        df_big = processDataset(path)\n",
    "        for year in year_list:\n",
    "            df = df_big[df_big.Year == year]\n",
    "            df_list_year[year] = df\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    for year, df in df_list_year.items():\n",
    "        # g = sns.lineplot(x=df['Month'], y=df['vertical_amount'], ax=ax, err_style=\"bars\", errorbar=(\"se\"), linestyle = linestyle_year[year], label=year)\n",
    "        g = sns.lineplot(x=df['Month'], y=df['vertical_amount'], ax=ax, err_style=\"bars\", errorbar=(\"se\"), c = color_dict[year], label=year)\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "    ax.set(ylim=(0, 0.00006))\n",
    "\n",
    "    g.set(xlabel='', \n",
    "        ylabel=r\"Vertical Column Amount [moles/m$\\mathregular{^2}$]\",\n",
    "        title= f'{chemical} Annual trend across {place} {year_list}')\n",
    "    if chemical == \"SO2\":\n",
    "        plt.ticklabel_format(style='plain', axis='y',useOffset=False)\n",
    "    \n",
    "    # plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/{chemical}/Annual trend {place}.png\"\n",
    "    # plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "color_dict = { 2021:\"red\",\n",
    "               2022: \"blue\",\n",
    "               2023:\"green\",\n",
    "               2024:\"black\"\n",
    "               }\n",
    "linestyle_year = {2021:\"-\",\n",
    "                  2022:\"--\",\n",
    "                  2023:\":\",\n",
    "                  2024:\"-.\",}\n",
    "\n",
    "\n",
    "# data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# places_list = [\"Mcmillan\"]\n",
    "# year_list = [2021, 2022, 2023, 2024]\n",
    "# for chemical in [\"SO2\"]:\n",
    "#         annualTrend(chemical, places_list, year_list)\n",
    "\n",
    "\n",
    "# data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# places_list = [\"BeltsVille\"]\n",
    "# year_list = [2023]\n",
    "# for chemical in [\"SO2\"]:\n",
    "#         annualTrend(chemical, places_list, year_list)\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "places_list = [\"Goddard\"]\n",
    "year_list = [2022, 2023]\n",
    "for chemical in [\"SO2\"]:\n",
    "        annualTrend(chemical, places_list, year_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Data Distribution & One-to-one line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(csvPath): # returns a df object\n",
    "    '''\n",
    "    Filtering: Three columns, High Quality Data\n",
    "            @ get three column datatime_UTC, quality, vertical_amount\n",
    "            @ filter data with just the high quality data\n",
    "\n",
    "    - change datetime_UTC to EST\n",
    "    - date, year, month, hour, time, rounded_hours\n",
    "    - get the date extracted\n",
    "    - get the year extracted\n",
    "    - get the month extracted\n",
    "    - get the hour extracted\n",
    "    - get the time extracted \n",
    "    - get the rounded hours extracted\n",
    "    ------> return the df with these things and do the analysis\n",
    "    '''\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    #############################\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time\n",
    "\n",
    "    # def round_to_nearest_hour(timestamp):\n",
    "\n",
    "    #     if timestamp.minute >= 30 or (timestamp.minute == 29 and timestamp.second >= 30):\n",
    "    #         # Round up\n",
    "    #         rounded_timestamp = timestamp + timedelta(hours=1)\n",
    "    #         rounded_timestamp = rounded_timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "    #     else:\n",
    "    #         # Round down\n",
    "    #         rounded_timestamp = timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "    #     return rounded_timestamp\n",
    "    # df.loc[:, \"roundedHour\"] = df['datetime_EST'].apply(round_to_nearest_hour).dt.hour   \n",
    "    return df\n",
    "\n",
    "def dataDistribution(chemical, year, places_list, t_test_places):\n",
    "    preprocessed_csv_path = {}\n",
    "    for place in places_list:\n",
    "        preprocessed_csv_path[place] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "    \n",
    "    df_list = {}\n",
    "    for place, path in preprocessed_csv_path.items():\n",
    "        df = processDataset(path)\n",
    "        if year == \"all\":\n",
    "            pass\n",
    "        else:\n",
    "            df = df[df.Year == year]\n",
    "        df_list[place] = df\n",
    "    \n",
    "    fig, axes = plt.subplots(len(places_list), 1, sharey=True, sharex=True, figsize=(16, 9))\n",
    "    ax = axes.flatten()\n",
    "    index = 0\n",
    "    \n",
    "    for place, df in df_list.items():\n",
    "        df_mean = df['vertical_amount'].mean()\n",
    "        df_median = df['vertical_amount'].median()\n",
    "        df_std = df['vertical_amount'].std()\n",
    "\n",
    "        g = sns.histplot(data = df, x = \"vertical_amount\", kde = True, ax = ax[index])\n",
    "        g.axvline(df_mean, color='k', linestyle=':', label='Mean')\n",
    "        g.axvline(df_median, color='r', linestyle='--', label='Median')\n",
    "        g.axvline(df_mean + df_std, color='g', linestyle='-', label='Mean + 1 SD')\n",
    "        g.axvline(df_mean - df_std, color='g', linestyle='-', label='Mean - 1 SD')\n",
    "        g.set_title(place)\n",
    "        g.legend()\n",
    "        index += 1\n",
    "\n",
    "    for x in ax:\n",
    "        x.set(xlabel='')\n",
    "        x.set(ylabel='')\n",
    "\n",
    "    ## t-test value for the things ###########################\n",
    "    x_axis_val = (g.get_xlim()[0] + g.get_xlim()[1]) / 2\n",
    "    y_axis_val = 2000\n",
    "    # x_axis_val, y_axis_val = 7000, 2000\n",
    "    for t_test_combination in t_test_places:\n",
    "        place1_verticalAmt_numpy = df_list[t_test_combination[0]][\"vertical_amount\"].to_numpy()\n",
    "        place2_verticalAmt_numpy = df_list[t_test_combination[1]][\"vertical_amount\"].to_numpy()\n",
    "        \n",
    "        t_stat, p_val = ttest_ind(place1_verticalAmt_numpy, place2_verticalAmt_numpy) \n",
    "        ax[1].text(x_axis_val, y_axis_val,f'{t_test_combination[0]} & {t_test_combination[1]} p-val: {\"{:.2e}\".format(p_val)}', fontsize=12) \n",
    "        y_axis_val -= 1500\n",
    "\n",
    "    fig.suptitle(f'{chemical} data points distributution for {year}')\n",
    "    fig.supxlabel(r\"Vertical Column Amount [moles/m$\\mathregular{^2}$]\")\n",
    "    fig.supylabel('Frequency Count')\n",
    "    \n",
    "    plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/{chemical}/Data points distributution for {year}.png\"\n",
    "    plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# chemical = \"H2O\"\n",
    "# year = \"all\"\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# t_test_places = [[\"Goddard\", \"Mcmillan\"], [\"Goddard\", \"BeltsVille\"]]\n",
    "# dataDistribution(chemical, year, places_list, t_test_places)\n",
    "\n",
    "\n",
    "places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "t_test_places = [[\"Goddard\", \"Mcmillan\"], [\"Goddard\", \"BeltsVille\"]]\n",
    "\n",
    "for chemical in [\"H2O\", \"HCHO\", \"NO2\", \"O3\", \"SO2\"]:\n",
    "        for year in [\"all\", 2023, 2024]:\n",
    "            dataDistribution(chemical, year, places_list, t_test_places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-to-one line comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_identity(axes, *line_args, **line_kwargs):\n",
    "    identity, = axes.plot([], [], *line_args, **line_kwargs)\n",
    "    def callback(axes):\n",
    "        low_x, high_x = axes.get_xlim()\n",
    "        low_y, high_y = axes.get_ylim()\n",
    "        low = max(low_x, low_y)\n",
    "        high = min(high_x, high_y)\n",
    "        identity.set_data([low, high], [low, high])\n",
    "    callback(axes)\n",
    "    axes.callbacks.connect('xlim_changed', callback)\n",
    "    axes.callbacks.connect('ylim_changed', callback)\n",
    "    return axes\n",
    "\n",
    "def processDataset(csvPath): # returns a df object\n",
    "    '''\n",
    "    Filtering: Three columns, High Quality Data\n",
    "            @ get three column datatime_UTC, quality, vertical_amount\n",
    "            @ filter data with just the high quality data\n",
    "\n",
    "    - change datetime_UTC to EST\n",
    "    - date, year, month, hour, time, rounded_hours\n",
    "    - get the date extracted\n",
    "    - get the year extracted\n",
    "    - get the month extracted\n",
    "    - get the hour extracted\n",
    "    - get the time extracted \n",
    "    - get the rounded hours extracted\n",
    "    ------> return the df with these things and do the analysis\n",
    "    '''\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    #############################\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time\n",
    "\n",
    "    df = df[['datetime_EST', 'vertical_amount', 'Year']]\n",
    "    df.set_index('datetime_EST', inplace=True)\n",
    "    df = df.resample('15T').mean()\n",
    "    df = df[df['vertical_amount'].notna()]\n",
    "    return df\n",
    "\n",
    "def one_to_one_comparison(chemical, year, places_list):\n",
    "    preprocessed_csv_path = {}\n",
    "    for place in places_list:\n",
    "        preprocessed_csv_path[place] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "    \n",
    "    df_list = {}\n",
    "    for place, path in preprocessed_csv_path.items():\n",
    "        df = processDataset(path)\n",
    "        if year == \"all\":\n",
    "            pass\n",
    "        else:\n",
    "            df = df[df.Year == year]\n",
    "        df_list[place] = df\n",
    "    \n",
    "    place1 = df_list[places_list[0]]\n",
    "    place2 = df_list[places_list[1]]\n",
    "\n",
    "    common_time_df = pd.merge(place1, place2, left_index=True, right_index=True, suffixes=(f'_{places_list[0]}', f'_{places_list[1]}'))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    g = sns.scatterplot(ax=ax,\n",
    "                        x = f\"vertical_amount_{places_list[0]}\",\n",
    "                        y = f\"vertical_amount_{places_list[1]}\",\n",
    "                        data= common_time_df)\n",
    "    \n",
    "    ## draw the diagnoal line\n",
    "    add_identity(ax, color='green', linewidth=4, label='one-to-one line')\n",
    "\n",
    "    # best fit line\n",
    "    plt.plot(np.unique(common_time_df[f'vertical_amount_{places_list[0]}']), \n",
    "        np.poly1d(np.polyfit(common_time_df[f'vertical_amount_{places_list[0]}'], \n",
    "        common_time_df[f'vertical_amount_{places_list[1]}'], 1))(np.unique(common_time_df[f'vertical_amount_{places_list[0]}'])),\n",
    "        color='black', linewidth=4, ls='--', label='best fit line')\n",
    "\n",
    "    plot_title = f'{chemical} {places_list[0]} vs {places_list[1]} comparison for {year} year'\n",
    "    g.set(xlabel= f'{places_list[0]} Vertical Column amount [moles/meter sq.]', \n",
    "        ylabel= f'{places_list[1]} Vertical Column amount [moles/meter sq.]',\n",
    "        title= plot_title)\n",
    "    plt.legend()\n",
    "\n",
    "    plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/{chemical}/1-1 comparison {places_list[0]} vs {places_list[1]} for {year} year.png\"\n",
    "    plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "\n",
    "    plt.show();    \n",
    "    \n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# chemical = \"H2O\"\n",
    "# year = \"all\"\n",
    "# places_list = [\"Goddard\", \"Mcmillan\"]\n",
    "# # places_list =[\"Goddard\", \"BeltsVille\"]\n",
    "# # places_list = [\"Mcmillan\", \"BeltsVille\"]\n",
    "# for places_list in [[\"Goddard\", \"Mcmillan\"], [\"Goddard\", \"BeltsVille\"], [\"Mcmillan\", \"BeltsVille\"]]:\n",
    "#     one_to_one_comparison(chemical, year, places_list)\n",
    "\n",
    "\n",
    "for places_list in [[\"Goddard\", \"Mcmillan\"], [\"Goddard\", \"BeltsVille\"], [\"Mcmillan\", \"BeltsVille\"]]:\n",
    "    for chemical in [\"H2O\", \"HCHO\", \"NO2\", \"O3\", \"SO2\"]:\n",
    "        for year in [\"all\", 2023, 2024]:\n",
    "            one_to_one_comparison(chemical, year, places_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(csvPath): # returns a df object\n",
    "    season_dict = {'January':'Winter',\n",
    "                'February':'Winter',\n",
    "                'March':'Winter',\n",
    "                'April':'Spring',\n",
    "                'May':'Spring',\n",
    "                'June':'Spring',\n",
    "                'July':'Summer',\n",
    "                'August':'Summer',\n",
    "                'September':'Summer',\n",
    "                'October':'Fall',\n",
    "                'November':'Fall',\n",
    "                'December':'Fall'}\n",
    "\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    #############################\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time\n",
    "    df['Season'] = df['Month'].map(season_dict)\n",
    "\n",
    "    def round_to_nearest_hour(timestamp):\n",
    "\n",
    "        if timestamp.minute >= 30 or (timestamp.minute == 29 and timestamp.second >= 30):\n",
    "            # Round up\n",
    "            rounded_timestamp = timestamp + timedelta(hours=1)\n",
    "            rounded_timestamp = rounded_timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "        else:\n",
    "            # Round down\n",
    "            rounded_timestamp = timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "        return rounded_timestamp\n",
    "    df.loc[:, \"roundedHour\"] = df['datetime_EST'].apply(round_to_nearest_hour).dt.hour   \n",
    "    return df\n",
    "\n",
    "def hourlyPlot(chemical, year, places_list):\n",
    "    preprocessed_csv_path = {}\n",
    "    for place in places_list:\n",
    "        preprocessed_csv_path[place] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "    \n",
    "    df_list = {}\n",
    "    for place, path in preprocessed_csv_path.items():\n",
    "        df = processDataset(path)\n",
    "        if year == \"all\":\n",
    "            pass\n",
    "        else:\n",
    "            df = df[df.Year == year]\n",
    "        df_list[place] = df\n",
    "        \n",
    "    for place, df in df_list.items():\n",
    "        fig, axes = plt.subplots(2, 2, sharey=True, figsize=(15, 10))\n",
    "        ax = axes.flatten()\n",
    "        for season in df.Season.unique():\n",
    "            df_particular_season_filterd = df[df.Season == season]\n",
    "            g = sns.boxplot(x='roundedHour', \n",
    "                            y='vertical_amount', \n",
    "                            data=df_particular_season_filterd, \n",
    "                            ax= ax[season_axes_dict[season]], \n",
    "                            color=color_dict[season])\n",
    "            ax[season_axes_dict[season]].set_title(season)\n",
    "        \n",
    "        # for removing the labels for each subplots\n",
    "        for x in ax:\n",
    "            x.set(xlabel='')\n",
    "            x.set(ylabel='')\n",
    "        \n",
    "        # whole subplots figures \n",
    "        fig.suptitle(f'{place} {chemical} Hourly plots for {year} year')\n",
    "        fig.supxlabel('EST Hours')\n",
    "        fig.supylabel(r\"Vertical Column Amount [moles/m$\\mathregular{^2}$]\")\n",
    "        \n",
    "        plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/{chemical}/Hourly plot {place} for {year} year.png\"\n",
    "        plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "        plt.show()  \n",
    "    \n",
    "\n",
    "season_axes_dict = {'Winter': 0, \n",
    "                    'Spring': 1, \n",
    "                    'Summer':2, \n",
    "                    'Fall': 3}\n",
    "\n",
    "color_dict = {'Winter': 'lightblue',\n",
    "              'Spring': 'yellow',\n",
    "              'Summer':'red',\n",
    "              'Fall': 'lightgreen'}\n",
    "\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# chemical = \"H2O\"\n",
    "# year = \"all\"\n",
    "# hourlyPlot(chemical, year, places_list)    \n",
    "\n",
    "\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# chemical = \"H2O\"\n",
    "# year = 2023\n",
    "# hourlyPlot(chemical, year, places_list) \n",
    "\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# chemical = \"H2O\"\n",
    "# year = 2024\n",
    "# hourlyPlot(chemical, year, places_list) \n",
    "\n",
    "places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "for chemical in [\"HCHO\", \"NO2\", \"O3\", \"SO2\"]:\n",
    "    for year in [\"all\", 2023, 2024]:\n",
    "        hourlyPlot(chemical, year, places_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(csvPath): # returns a df object\n",
    "    season_dict = {'January':'Winter',\n",
    "                'February':'Winter',\n",
    "                'March':'Winter',\n",
    "                'April':'Spring',\n",
    "                'May':'Spring',\n",
    "                'June':'Spring',\n",
    "                'July':'Summer',\n",
    "                'August':'Summer',\n",
    "                'September':'Summer',\n",
    "                'October':'Fall',\n",
    "                'November':'Fall',\n",
    "                'December':'Fall'}\n",
    "\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    #############################\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time\n",
    "    df['Season'] = df['Month'].map(season_dict)\n",
    "\n",
    "    def round_to_nearest_hour(timestamp):\n",
    "\n",
    "        if timestamp.minute >= 30 or (timestamp.minute == 29 and timestamp.second >= 30):\n",
    "            # Round up\n",
    "            rounded_timestamp = timestamp + timedelta(hours=1)\n",
    "            rounded_timestamp = rounded_timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "        else:\n",
    "            # Round down\n",
    "            rounded_timestamp = timestamp.replace(minute=0, second=0, microsecond=0)\n",
    "        return rounded_timestamp\n",
    "    df.loc[:, \"roundedHour\"] = df['datetime_EST'].apply(round_to_nearest_hour).dt.hour   \n",
    "    return df\n",
    "\n",
    "def seasonalityVariation(chemical, year, places_list):\n",
    "    preprocessed_csv_path = {}\n",
    "    for place in places_list:\n",
    "        preprocessed_csv_path[place] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "    \n",
    "    df_list = {}\n",
    "    for place, path in preprocessed_csv_path.items():\n",
    "        df = processDataset(path)\n",
    "        if year == \"all\":\n",
    "            pass\n",
    "        else:\n",
    "            df = df[df.Year == year]\n",
    "        df_list[place] = df\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, sharey=True, sharex=True, figsize=(20, 15))\n",
    "    ax = axes.flatten()\n",
    "    index = 0        \n",
    "    for place, df in df_list.items():\n",
    "        g = sns.boxplot(data = df, x = \"Season\", y = 'vertical_amount', showmeans = True, ax = ax[index])\n",
    "        g.set_title(place)\n",
    "        g.legend()\n",
    "        index += 1\n",
    "    \n",
    "    for x in ax:\n",
    "        x.set(xlabel='')\n",
    "        x.set(ylabel='')  \n",
    "        # x.set_ylim(0, 8000)  \n",
    "    \n",
    "    fig.suptitle(f'Season Comparison {chemical} for {year} year')\n",
    "    fig.supylabel(r\"Vertical Column Amount [moles/m$\\mathregular{^2}$]\")\n",
    "\n",
    "    plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/{chemical}/Season comparison for {year} year.png\"\n",
    "    plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# chemical = \"H2O\"\n",
    "# year = 2023\n",
    "# seasonalityVariation(chemical, year, places_list) \n",
    "\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# chemical = \"H2O\"\n",
    "# year = 2024\n",
    "# seasonalityVariation(chemical, year, places_list) \n",
    "\n",
    "# places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "# chemical = \"H2O\"\n",
    "# year = \"all\"\n",
    "# seasonalityVariation(chemical, year, places_list) \n",
    "\n",
    "\n",
    "places_list = [\"BeltsVille\", \"Goddard\", \"Mcmillan\"]\n",
    "for chemical in [\"HCHO\", \"NO2\", \"O3\", \"SO2\"]:\n",
    "    for year in [\"all\", 2023, 2024]:\n",
    "        seasonalityVariation(chemical, year, places_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corelation Analysis\n",
    "\n",
    "* I did a correlation analysis for Water vapor and Fomaldeyde to know what is the thing that is going on with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(csvPath): # returns a df object\n",
    "    df = pd.read_csv(csvPath,  parse_dates=['datetime_UTC'])\n",
    "    # Filtering\n",
    "    workingColumns = ['datetime_UTC', 'quality', 'vertical_amount']\n",
    "    df = df[workingColumns]\n",
    "    df = df[df.quality == 'high quality na']\n",
    "    #############################\n",
    "    \n",
    "    df['datetime_EST'] = df['datetime_UTC'].dt.tz_localize('UTC').dt.tz_convert('America/New_York') # change into EST\n",
    "    # Extract date, year, month, hour, time, rounded_hours\n",
    "    df.loc[:, 'Date'] = df['datetime_EST'].dt.date\n",
    "    df.loc[:, 'Year'] = df['datetime_EST'].dt.year\n",
    "    df.loc[:, 'Month'] = df['datetime_EST'].dt.month_name()\n",
    "    df.loc[:, 'Hour'] = df['datetime_EST'].dt.hour\n",
    "    df.loc[:, 'Time'] = df['datetime_EST'].dt.time\n",
    "\n",
    "    df = df[['datetime_EST', 'vertical_amount', 'Year']]\n",
    "    df.set_index('datetime_EST', inplace=True)\n",
    "    df = df.resample('15T').mean()\n",
    "    df = df[df['vertical_amount'].notna()]\n",
    "    return df\n",
    "\n",
    "def corelation_value(year, place, chemicals):\n",
    "    preprocessed_csv_path = {}\n",
    "    for chemical in chemicals:\n",
    "        preprocessed_csv_path[chemical] = f\"{data_folder_path}/{chemical}_{place}.csv\"\n",
    "    \n",
    "    df_list = {}\n",
    "    for chemical, path in preprocessed_csv_path.items():\n",
    "        df = processDataset(path)\n",
    "        if year == \"all\":\n",
    "            pass\n",
    "        else:\n",
    "            df = df[df.Year == year]\n",
    "        df_list[chemical] = df\n",
    "    \n",
    "    chemical0 = chemicals[0]\n",
    "    chemical1 = chemicals[1]\n",
    "\n",
    "    df_H2O = df_list[chemical0]\n",
    "    df_HCHO = df_list[chemical1]\n",
    "    merged = pd.merge(df_H2O, df_HCHO, left_index=True, right_index=True, how='inner', suffixes=(f'_{chemical0}', f'_{chemical1}'))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    g = sns.scatterplot(ax=ax,\n",
    "                        x = f\"vertical_amount_{chemical0}\",\n",
    "                        y = f\"vertical_amount_{chemical1}\",\n",
    "                        data= merged)\n",
    "    \n",
    "    correlation = merged[f\"vertical_amount_{chemical0}\"].corr(merged[f\"vertical_amount_{chemical1}\"])\n",
    "    ax.text(0.3, 0.8,\n",
    "            f\"Pearson Correlation: {correlation:.2f}\",\n",
    "            fontsize=20, \n",
    "            ha='center', va='center', transform=ax.transAxes) \n",
    "\n",
    "    # best fit line\n",
    "    plt.plot(np.unique(merged[f\"vertical_amount_{chemical0}\"]), \n",
    "        np.poly1d(np.polyfit(merged[f\"vertical_amount_{chemical0}\"], \n",
    "        merged[f\"vertical_amount_{chemical1}\"], 1))(np.unique(merged[f\"vertical_amount_{chemical0}\"])),\n",
    "        color='black', linewidth=4, ls='--', label='best fit line')\n",
    "\n",
    "    plot_title = f'Comparison of {chemical0} vs {chemical1} in {place} for {year}'\n",
    "\n",
    "\n",
    "    x_label = \"moles/m$\\\\mathregular{^2}$\"\n",
    "    y_label = \"moles/m$\\\\mathregular{^2}$\"\n",
    "\n",
    "    g.set(xlabel= f\"{place} {chemical0} Column Amount [{x_label}]\", \n",
    "        ylabel= f\"{place} {chemical1} Column Amount [{y_label}]\",\n",
    "        title= plot_title)\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plotSavePath = f\"/Users/ujjawal/Documents/Work/PandoraWork_2025/Plots/allCommon/{chemical0} vs {chemical1} {place} {year}.png\"\n",
    "    plt.savefig(plotSavePath, bbox_inches='tight');\n",
    "    plt.show(); \n",
    "\n",
    "# For H2O vs HCHO\n",
    "'''\n",
    "# data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# place = \"Mcmillan\"\n",
    "# chemicals = [\"H2O\", \"HCHO\"]\n",
    "# for year in [2021, 2022, 2023, 2024]:\n",
    "#     corelation_value(year, place, chemicals)\n",
    "\n",
    "\n",
    "# data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "# place = \"BeltsVille\"\n",
    "# chemicals = [\"H2O\", \"HCHO\"]\n",
    "# for year in [2023]:\n",
    "#     corelation_value(year, place, chemicals)\n",
    "'''\n",
    "\n",
    "\n",
    "# For H2O vs SO2\n",
    "'''\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "place = \"Mcmillan\"\n",
    "chemicals = [\"H2O\", \"SO2\"]\n",
    "for year in [2021, 2022, 2023, 2024]:\n",
    "    corelation_value(year, place, chemicals)\n",
    "\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "place = \"BeltsVille\"\n",
    "chemicals = [\"H2O\", \"SO2\"]\n",
    "for year in [2023]:\n",
    "    corelation_value(year, place, chemicals)\n",
    "'''\n",
    "\n",
    "\n",
    "# For HCHO vs SO2\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "place = \"Mcmillan\"\n",
    "chemicals = [\"HCHO\", \"SO2\"]\n",
    "for year in [2021, 2022, 2023, 2024]:\n",
    "    corelation_value(year, place, chemicals)\n",
    "\n",
    "\n",
    "data_folder_path = \"/Users/ujjawal/Documents/Work/PandoraWork_2025/Data\"\n",
    "place = \"Goddard\"\n",
    "chemicals = [\"HCHO\", \"SO2\"]\n",
    "for year in [2022, 2023]:\n",
    "    corelation_value(year, place, chemicals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
